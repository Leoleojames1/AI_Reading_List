# Borch's Top Resources on Cybersecurity for AI & Agentic Software

## [Purple Llama](https://github.com/meta-llama/PurpleLlama/tree/main)

Purple Llama is an umbrella project that over time will bring together tools and evals to help the community build responsibly with open generative AI models. The initial release will include tools and evals for Cyber Security and Input/Output safeguards but we plan to contribute more in the near future. [(prompt injection benchmark)](https://github.com/meta-llama/PurpleLlama/blob/main/CybersecurityBenchmarks/benchmark/prompt_injection_benchmark.py)

## [Critical RCE Vulnerability Discovered in Ollama AI Infrastructure Tool](https://thehackernews.com/2024/06/critical-rce-vulnerability-discovered.html)

A new vulnerability "Probllama" has put AI servers at risk, allowing attackers to execute remote code in exposed Ollama instances. Learn how this flaw, targeting AI infrastructure, was quickly patchedâ€”but exposed over 1,000 servers to critical risks. Dive into the world of AI security to understand how classic vulnerabilities threaten modern AI platforms.

## [ðŸ¤¯ AI Security EXPOSED! Hidden Risks of AI Agents â€“ Shai Alon, Orca Security // TechSpot](https://www.youtube.com/watch?v=jT6sTw8Cr7U&list=PLO30wIj8QSRDxDvYzKYeBttEwIs2jI56W&index=105&ab_channel=OnTheSpotDevelopment)

In this insightful presentation, Shai Alon from Orca Security explores the emerging security challenges posed by AI applications and agents. Key highlights include:

Prompt injection: Manipulating AI responses with malicious inputs
Data poisoning: Contaminating training data to influence AI behavior
Unauthorized access through AI-generated queries

Security concerns in four main categories: employee AI tool usage, AI sprawl in cloud environments, AI models, and AI app business logic.
Demonstration of AI agents writing and executing code, including SQL queries, presenting new security risks.
Discussion of the OWASP Top 10 for Large Language Model (LLM) applications.
Exploration of new attack vectors like model denial of service and AI supply chain vulnerabilities.
Emphasis on the need for improved security measures in AI development, including:

Better input validation and sanitization
Proper access controls and permissions
Secure sandboxing for AI-generated code execution

## [NEW AI Jailbreak Method SHATTERS GPT4, Claude, Gemini, LLaMA](https://www.youtube.com/watch?v=5cEvNO9rZgI&list=PLO30wIj8QSRDxDvYzKYeBttEwIs2jI56W&index=14&ab_channel=MatthewBerman)

A new AI jailbreak technique uses ASCII art to fool advanced models like GPT-4 and Claude into bypassing content filters. Watch the video to discover how this technique works, how the creator tests it live, and the breakthrough using Morse code that shatters even the most secure language models. Unveil the power and risks of model jailbreaking!

## [NEW Universal AI Jailbreak SMASHES GPT4, Claude, Gemini, LLaMA ](https://www.youtube.com/watch?v=9IM5d-egZ7M&t=584s)

## [Llama 405b BEAST already exploited | Hereâ€™s how](https://www.youtube.com/watch?v=zEzWWUddn34&ab_channel=AlexZiskind)

## [Injecting AI (LLM): The Terminal - Episode 1](https://www.youtube.com/watch?v=ju8hdBaNDpI)

## [Learn AI Engineer Skills: Autonomous Agentic Behavior (Llama 3 8B Ollama)](https://www.youtube.com/watch?v=pCe0xWrtChY&ab_channel=AllAboutAI)

## [How Good is Your Prompt Engineering? LLM Hacker Challenge With Bounty Reward](https://www.youtube.com/watch?v=oe2PhEyERzc&ab_channel=AllAboutAI)
